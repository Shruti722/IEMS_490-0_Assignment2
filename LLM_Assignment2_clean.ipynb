{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7nhswzM2kzmZ",
    "outputId": "89080558-2e86-4955-c56d-dadfcf1cf04a"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "ASSIGN_DIR = \"/content/drive/MyDrive/LLM_Assignment_2\"\n",
    "!mkdir -p \"$ASSIGN_DIR\"\n",
    "%cd \"$ASSIGN_DIR\"\n",
    "!echo \"Working in: $PWD\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5YxdFviEdv1c",
    "outputId": "0ce5096c-2931-4cc9-ff55-306faff19bad"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip -q install \"transformers>=4.45.0\" \"accelerate>=0.34.0\" \"datasets>=2.20.0\" \\\n",
    "               \"peft>=0.13.0\" \"evaluate>=0.4.2\" \"bitsandbytes>=0.43.1\" \\\n",
    "               \"torch>=2.3.0\"\n",
    "python - << 'PY'\n",
    "import torch, platform\n",
    "print(\"Torch:\", torch.__version__, \"| CUDA:\", torch.cuda.is_available(), \"| GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else None)\n",
    "print(platform.platform())\n",
    "PY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qLZuB7mieIT_",
    "outputId": "e981e05a-46fa-4de8-b081-8110b4d21e36"
   },
   "outputs": [],
   "source": [
    "%%writefile train_lora.py\n",
    "import os, json, torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling,\n",
    "    Trainer, TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# ---------- ENV (edit via os.environ in Colab cells) ----------\n",
    "BASE_MODEL = os.environ.get(\"BASE_MODEL\", \"HuggingFaceTB/SmolLM2-360M-Instruct\")\n",
    "TRAIN_PATH = os.environ.get(\"TRAIN_PATH\", \"data/train.jsonl\")\n",
    "VAL_PATH   = os.environ.get(\"VAL_PATH\",   \"data/val.jsonl\")\n",
    "OUTPUT_DIR = os.environ.get(\"OUTPUT_DIR\", \"adapters/task-lora\")\n",
    "USE_4BIT   = os.environ.get(\"USE_4BIT\", \"1\") == \"1\"  # helps memory on T4 (16GB)\n",
    "\n",
    "LORA_R        = int(os.environ.get(\"LORA_R\", \"16\"))\n",
    "LORA_ALPHA    = int(os.environ.get(\"LORA_ALPHA\", \"32\"))\n",
    "LORA_DROPOUT  = float(os.environ.get(\"LORA_DROPOUT\", \"0.05\"))\n",
    "\n",
    "LR            = float(os.environ.get(\"LR\", \"2e-4\"))\n",
    "EPOCHS        = float(os.environ.get(\"EPOCHS\", \"1\"))\n",
    "BATCH_SIZE    = int(os.environ.get(\"BATCH_SIZE\", \"8\"))\n",
    "GRAD_ACCUM    = int(os.environ.get(\"GRAD_ACCUM\", \"2\"))\n",
    "MAX_STEPS     = int(os.environ.get(\"MAX_STEPS\", \"0\"))    # 0 => use epochs\n",
    "WARMUP_RATIO  = float(os.environ.get(\"WARMUP_RATIO\", \"0.03\"))\n",
    "LOG_STEPS     = int(os.environ.get(\"LOG_STEPS\", \"10\"))\n",
    "SAVE_STEPS    = int(os.environ.get(\"SAVE_STEPS\", \"200\"))\n",
    "SEED          = int(os.environ.get(\"SEED\", \"42\"))\n",
    "\n",
    "TARGET_MODULES = os.environ.get(\n",
    "    \"TARGET_MODULES\",\n",
    "    \"q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj\"\n",
    ").split(\",\")\n",
    "\n",
    "PROMPT_TEMPLATE = os.environ.get(\"PROMPT_TEMPLATE\", \"\"\"\\\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "\"\"\")\n",
    "\n",
    "def format_row(ex):\n",
    "    instruction = (ex.get(\"instruction\") or \"\").strip()\n",
    "    inp = (ex.get(\"input\") or \"\").strip()\n",
    "    out = (ex.get(\"output\") or \"\").strip()\n",
    "    return {\"text\": PROMPT_TEMPLATE.format(instruction=instruction, input=inp) + out}\n",
    "\n",
    "def load_jsonl_dataset(train_path, val_path):\n",
    "    ds = load_dataset(\"json\", data_files={\"train\": train_path, \"validation\": val_path})\n",
    "    ds = ds.map(format_row, remove_columns=ds[\"train\"].column_names)\n",
    "    return ds\n",
    "\n",
    "def gpu_dtype():\n",
    "    # T4 prefers fp16 (no bf16). Use fp16 on CUDA else fp32.\n",
    "    return torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "def main():\n",
    "    torch.manual_seed(SEED)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    quant_kwargs = {}\n",
    "    if USE_4BIT:\n",
    "        from transformers import BitsAndBytesConfig\n",
    "        bnb = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=gpu_dtype(),\n",
    "        )\n",
    "        quant_kwargs[\"quantization_config\"] = bnb\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        dtype=gpu_dtype(),          # <- updated (replaces deprecated torch_dtype)\n",
    "        device_map=\"auto\",\n",
    "        **quant_kwargs\n",
    "    )\n",
    "\n",
    "    lora = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=LORA_R,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        target_modules=TARGET_MODULES,\n",
    "        bias=\"none\",\n",
    "    )\n",
    "    model = get_peft_model(model, lora)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    ds = load_jsonl_dataset(TRAIN_PATH, VAL_PATH)\n",
    "\n",
    "    def tok(batch):\n",
    "        return tokenizer(batch[\"text\"], truncation=True, max_length=1024, padding=\"max_length\")\n",
    "    tokenized = ds.map(tok, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=min(4, BATCH_SIZE),\n",
    "        gradient_accumulation_steps=GRAD_ACCUM,\n",
    "        learning_rate=LR,\n",
    "        warmup_ratio=WARMUP_RATIO,\n",
    "        logging_steps=LOG_STEPS,\n",
    "        save_steps=SAVE_STEPS,\n",
    "        eval_strategy=\"steps\",     # <- updated arg name for recent Transformers\n",
    "        evaluation_strategy=\"no\",  # keep backward-safe default off\n",
    "        eval_steps=SAVE_STEPS,\n",
    "        report_to=\"none\",\n",
    "        gradient_checkpointing=True,\n",
    "        optim=\"paged_adamw_8bit\" if USE_4BIT else \"adamw_torch\",\n",
    "        seed=SEED,\n",
    "        bf16=False,                 # T4: keep bf16 off\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        max_steps=MAX_STEPS,        # <- always int; 0 = use epochs\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=tokenized[\"train\"],\n",
    "        eval_dataset=tokenized[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    model.save_pretrained(OUTPUT_DIR)\n",
    "    print(f\"Saved LoRA adapter to {OUTPUT_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n_gGRtV0eCvt",
    "outputId": "1a132226-ae90-41ac-e00e-d8e556e423cb"
   },
   "outputs": [],
   "source": [
    "%%writefile infer.py\n",
    "import os, json, torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "EVAL_FILE = os.environ.get(\"EVAL_FILE\", \"data/eval_questions.jsonl\")\n",
    "BASE_MODEL = os.environ.get(\"BASE_MODEL\", \"HuggingFaceTB/SmolLM2-360M-Instruct\")\n",
    "PEFT_ADAPTER_PATH = os.environ.get(\"PEFT_ADAPTER_PATH\", \"\")\n",
    "\n",
    "MAX_NEW_TOKENS = int(os.environ.get(\"MAX_NEW_TOKENS\", \"192\"))\n",
    "TEMPERATURE = float(os.environ.get(\"TEMPERATURE\", \"0.2\"))\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\\\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "def gpu_dtype():\n",
    "    return torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "def generate(model, tokenizer, prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=True,\n",
    "            temperature=TEMPERATURE,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    return text.split(\"### Response:\")[-1].strip()\n",
    "\n",
    "def main():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL, torch_dtype=gpu_dtype(), device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    if PEFT_ADAPTER_PATH:\n",
    "        print(f\"[infer] Using LoRA adapter: {PEFT_ADAPTER_PATH}\")\n",
    "        model = PeftModel.from_pretrained(model, PEFT_ADAPTER_PATH)\n",
    "\n",
    "    with open(EVAL_FILE, \"r\") as f:\n",
    "        rows = [json.loads(l) for l in f]\n",
    "\n",
    "    os.makedirs(\"outputs\", exist_ok=True)\n",
    "    out_path = \"outputs/finetuned_responses.jsonl\" if PEFT_ADAPTER_PATH else \"outputs/base_responses.jsonl\"\n",
    "    with open(out_path, \"w\") as w:\n",
    "        for r in rows:\n",
    "            prompt = PROMPT_TEMPLATE.format(\n",
    "                instruction=r.get(\"instruction\",\"\"),\n",
    "                input=r.get(\"input\",\"\"),\n",
    "            )\n",
    "            resp = generate(model, tokenizer, prompt)\n",
    "            w.write(json.dumps({\"instruction\": r.get(\"instruction\",\"\"),\n",
    "                                \"input\": r.get(\"input\",\"\"),\n",
    "                                \"response\": resp}, ensure_ascii=False) + \"\\n\")\n",
    "    print(f\"Wrote {len(rows)} responses to {out_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ay1pU0FieQD5",
    "outputId": "8068ee91-c77d-4df5-d212-6899a875c6b1"
   },
   "outputs": [],
   "source": [
    "%%writefile unit_test.py\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "# Tiny synthetic sentiment set (also works if you choose summarization later)\n",
    "TINY_TRAIN = [{\"instruction\":\"Classify sentiment as POS or NEG.\",\"input\":x,\"output\":y} for x,y in [\n",
    "    (\"I love this!\", \"POS\"),\n",
    "    (\"Terrible product.\", \"NEG\"),\n",
    "    (\"Absolutely fantastic experience.\", \"POS\"),\n",
    "    (\"Not worth the money.\", \"NEG\"),\n",
    "]] * 5  # 20 rows\n",
    "TINY_VAL = [\n",
    "    {\"instruction\":\"Classify sentiment as POS or NEG.\",\"input\":\"This makes me happy.\",\"output\":\"POS\"},\n",
    "    {\"instruction\":\"Classify sentiment as POS or NEG.\",\"input\":\"I am disappointed.\",\"output\":\"NEG\"},\n",
    "]\n",
    "\n",
    "def write_jsonl(p, rows):\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(p, \"w\") as f:\n",
    "        for r in rows: f.write(json.dumps(r) + \"\\n\")\n",
    "\n",
    "def main():\n",
    "    train = Path(\"tmp_unit/train.jsonl\")\n",
    "    val   = Path(\"tmp_unit/val.jsonl\")\n",
    "    write_jsonl(train, TINY_TRAIN)\n",
    "    write_jsonl(val,   TINY_VAL)\n",
    "\n",
    "    env = os.environ.copy()\n",
    "    env.update({\n",
    "        \"TRAIN_PATH\": str(train),\n",
    "        \"VAL_PATH\": str(val),\n",
    "        \"OUTPUT_DIR\": \"adapters/_unit_lora\",\n",
    "        \"USE_4BIT\": \"1\",\n",
    "        \"LORA_R\": \"8\",\n",
    "        \"LORA_ALPHA\": \"16\",\n",
    "        \"LORA_DROPOUT\": \"0.05\",\n",
    "        \"LR\": \"3e-4\",\n",
    "        \"EPOCHS\": \"1\",\n",
    "        \"BATCH_SIZE\": \"8\",\n",
    "        \"GRAD_ACCUM\": \"1\",\n",
    "        \"MAX_STEPS\": \"20\",\n",
    "        \"SAVE_STEPS\": \"50\",\n",
    "        \"LOG_STEPS\": \"5\",\n",
    "    })\n",
    "    print(\"[unit] Starting short LoRA training...\")\n",
    "    subprocess.run([\"python\", \"train_lora.py\"], check=True, env=env)\n",
    "    assert Path(\"adapters/_unit_lora\").exists()\n",
    "\n",
    "    env2 = os.environ.copy()\n",
    "    env2.update({\n",
    "        \"PEFT_ADAPTER_PATH\": \"adapters/_unit_lora\",\n",
    "        \"EVAL_FILE\": str(val),\n",
    "    })\n",
    "    subprocess.run([\"python\", \"infer.py\"], check=True, env=env2)\n",
    "    print(\"[unit] OK.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nx-s1y0AeUAh"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p data\n",
    "cat > data/train.jsonl << 'JSON'\n",
    "{\"instruction\":\"Classify sentiment as POS or NEG.\",\"input\":\"This phone is amazing, great battery life!\",\"output\":\"POS\"}\n",
    "{\"instruction\":\"Classify sentiment as POS or NEG.\",\"input\":\"Worst purchase ever.\",\"output\":\"NEG\"}\n",
    "{\"instruction\":\"Classify sentiment as POS or NEG.\",\"input\":\"Pretty decent for the price.\",\"output\":\"POS\"}\n",
    "{\"instruction\":\"Classify sentiment as POS or NEG.\",\"input\":\"It broke after a week, I'm upset.\",\"output\":\"NEG\"}\n",
    "JSON\n",
    "\n",
    "cat > data/val.jsonl << 'JSON'\n",
    "{\"instruction\":\"Classify sentiment as POS or NEG.\",\"input\":\"I enjoy using this every day.\",\"output\":\"POS\"}\n",
    "{\"instruction\":\"Classify sentiment as POS or NEG.\",\"input\":\"Support was unhelpful and rude.\",\"output\":\"NEG\"}\n",
    "JSON\n",
    "\n",
    "# Held-out eval questions (no gold labels needed)\n",
    "cat > data/eval_questions.jsonl << 'JSON'\n",
    "{\"instruction\":\"Classify sentiment as POS or NEG.\",\"input\":\"The update made it so much better.\"}\n",
    "{\"instruction\":\"Classify sentiment as POS or NEG.\",\"input\":\"Totally disappointed with the quality.\"}\n",
    "JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pFpdO1ZCefVC",
    "outputId": "1dd1649f-7ead-49e5-f187-c202797ea0e2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"BASE_MODEL\"] = \"HuggingFaceTB/SmolLM2-360M-Instruct\"  # safe default\n",
    "os.environ[\"EVAL_FILE\"] = \"data/eval_questions.jsonl\"\n",
    "\n",
    "# Base outputs → outputs/base_responses.jsonl\n",
    "!python infer.py\n",
    "!sed -n '1,5p' outputs/base_responses.jsonl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11n6XR2qejhb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"BASE_MODEL\"] = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
    "os.environ[\"TRAIN_PATH\"] = \"data/train.jsonl\"\n",
    "os.environ[\"VAL_PATH\"]   = \"data/val.jsonl\"\n",
    "os.environ[\"OUTPUT_DIR\"] = \"adapters/task-lora\"\n",
    "os.environ[\"USE_4BIT\"]   = \"1\"     # keeps VRAM low on T4\n",
    "os.environ[\"LORA_R\"]     = \"16\"\n",
    "os.environ[\"LORA_ALPHA\"] = \"32\"\n",
    "os.environ[\"LORA_DROPOUT\"] = \"0.05\"\n",
    "os.environ[\"LR\"] = \"2e-4\"\n",
    "os.environ[\"EPOCHS\"] = \"1\"         # increase later for better results\n",
    "os.environ[\"BATCH_SIZE\"] = \"8\"\n",
    "os.environ[\"GRAD_ACCUM\"] = \"2\"\n",
    "os.environ[\"SAVE_STEPS\"] = \"200\"\n",
    "os.environ[\"LOG_STEPS\"]  = \"10\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TsCp2GWCgwrv",
    "outputId": "90b81ba1-ca6d-41af-a33e-846529f343f6"
   },
   "outputs": [],
   "source": [
    "# From your assignment folder\n",
    "%cd /content/drive/MyDrive/LLM_Assignment_2\n",
    "\n",
    "# Patch train_lora.py to enable k-bit training prep\n",
    "import re, io, sys\n",
    "path = \"train_lora.py\"\n",
    "src = open(path, \"r\").read()\n",
    "\n",
    "# 1) Ensure import\n",
    "if \"prepare_model_for_kbit_training\" not in src:\n",
    "    src = src.replace(\n",
    "        \"from peft import LoraConfig, get_peft_model, TaskType\",\n",
    "        \"from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\"\n",
    "    )\n",
    "\n",
    "# 2) After model = AutoModelForCausalLM.from_pretrained(...), call prepare_model_for_kbit_training(model)\n",
    "src = src.replace(\n",
    "    \"model = AutoModelForCausalLM.from_pretrained(\",\n",
    "    \"model = AutoModelForCausalLM.from_pretrained(\"\n",
    ")\n",
    "\n",
    "# Insert the prepare call only if not already present\n",
    "if \"prepare_model_for_kbit_training(model)\" not in src:\n",
    "    src = src.replace(\n",
    "        \")\\\\n\\\\n    lora = LoraConfig(\",\n",
    "        \")\\n\\n    # Prepare model for 4/8-bit (enables gradients in a safe way)\\n\"\n",
    "        \"    if USE_4BIT:\\n\"\n",
    "        \"        model = prepare_model_for_kbit_training(model)\\n\"\n",
    "        \"    # Disable cache for training (checkpointing-friendly)\\n\"\n",
    "        \"    if hasattr(model.config, 'use_cache'):\\n\"\n",
    "        \"        model.config.use_cache = False\\n\\n\"\n",
    "        \"    lora = LoraConfig(\"\n",
    "    )\n",
    "\n",
    "open(path, \"w\").write(src)\n",
    "print(\"Patched train_lora.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x5Ftzl6HhOyB",
    "outputId": "5f47d625-5d1e-4798-84de-506f1ca932b4"
   },
   "outputs": [],
   "source": [
    "%%writefile /content/drive/MyDrive/LLM_Assignment_2/train_lora.py\n",
    "import os, torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments,\n",
    "    default_data_collator\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "\n",
    "# ---------- ENV ----------\n",
    "BASE_MODEL = os.environ.get(\"BASE_MODEL\", \"HuggingFaceTB/SmolLM2-360M-Instruct\")\n",
    "TRAIN_PATH = os.environ.get(\"TRAIN_PATH\", \"data/train.jsonl\")\n",
    "VAL_PATH   = os.environ.get(\"VAL_PATH\",   \"data/val.jsonl\")\n",
    "OUTPUT_DIR = os.environ.get(\"OUTPUT_DIR\", \"adapters/task-lora\")\n",
    "USE_4BIT   = os.environ.get(\"USE_4BIT\", \"1\") == \"1\"  # T4-friendly\n",
    "\n",
    "LORA_R        = int(os.environ.get(\"LORA_R\", \"16\"))\n",
    "LORA_ALPHA    = int(os.environ.get(\"LORA_ALPHA\", \"32\"))\n",
    "LORA_DROPOUT  = float(os.environ.get(\"LORA_DROPOUT\", \"0.05\"))\n",
    "\n",
    "LR            = float(os.environ.get(\"LR\", \"2e-4\"))\n",
    "EPOCHS        = float(os.environ.get(\"EPOCHS\", \"1\"))\n",
    "BATCH_SIZE    = int(os.environ.get(\"BATCH_SIZE\", \"8\"))\n",
    "GRAD_ACCUM    = int(os.environ.get(\"GRAD_ACCUM\", \"2\"))\n",
    "MAX_STEPS     = int(os.environ.get(\"MAX_STEPS\", \"0\"))    # 0 => use epochs\n",
    "WARMUP_RATIO  = float(os.environ.get(\"WARMUP_RATIO\", \"0.03\"))\n",
    "LOG_STEPS     = int(os.environ.get(\"LOG_STEPS\", \"10\"))\n",
    "SAVE_STEPS    = int(os.environ.get(\"SAVE_STEPS\", \"200\"))\n",
    "SEED          = int(os.environ.get(\"SEED\", \"42\"))\n",
    "\n",
    "TARGET_MODULES = os.environ.get(\n",
    "    \"TARGET_MODULES\",\n",
    "    \"q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj\"\n",
    ").split(\",\")\n",
    "\n",
    "PROMPT_TEMPLATE = os.environ.get(\"PROMPT_TEMPLATE\", \"\"\"\\\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "\"\"\")\n",
    "\n",
    "def gpu_dtype():\n",
    "    # T4: use fp16; CPU: fp32\n",
    "    return torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "def format_row(ex):\n",
    "    instruction = (ex.get(\"instruction\") or \"\").strip()\n",
    "    inp = (ex.get(\"input\") or \"\").strip()\n",
    "    out = (ex.get(\"output\") or \"\").strip()\n",
    "    return {\"text\": PROMPT_TEMPLATE.format(instruction=instruction, input=inp) + out}\n",
    "\n",
    "def load_jsonl_dataset(train_path, val_path):\n",
    "    ds = load_dataset(\"json\", data_files={\"train\": train_path, \"validation\": val_path})\n",
    "    ds = ds.map(format_row, remove_columns=ds[\"train\"].column_names)\n",
    "    return ds\n",
    "\n",
    "def main():\n",
    "    torch.manual_seed(SEED)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    quant_kwargs = {}\n",
    "    if USE_4BIT:\n",
    "        from transformers import BitsAndBytesConfig\n",
    "        bnb = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=gpu_dtype(),\n",
    "        )\n",
    "        quant_kwargs[\"quantization_config\"] = bnb\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        dtype=gpu_dtype(),\n",
    "        device_map=\"auto\",\n",
    "        **quant_kwargs\n",
    "    )\n",
    "\n",
    "    # K-bit prep + checkpointing-friendly flags\n",
    "    if USE_4BIT:\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "        # Explicitly ensure inputs can require grads under GC paths\n",
    "        if hasattr(model, \"enable_input_require_grads\"):\n",
    "            model.enable_input_require_grads()\n",
    "    if hasattr(model.config, \"use_cache\"):\n",
    "        model.config.use_cache = False\n",
    "    # Extra safety (Trainer also toggles this with gradient_checkpointing=True)\n",
    "    if hasattr(model, \"gradient_checkpointing_enable\"):\n",
    "        model.gradient_checkpointing_enable()\n",
    "\n",
    "    # LoRA wrap\n",
    "    lora = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=LORA_R,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        target_modules=TARGET_MODULES,\n",
    "        bias=\"none\",\n",
    "    )\n",
    "    model = get_peft_model(model, lora)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    # Data\n",
    "    ds = load_jsonl_dataset(TRAIN_PATH, VAL_PATH)\n",
    "\n",
    "    # Tokenize + create labels directly (so loss always hooks into graph)\n",
    "    def tok(batch):\n",
    "        enc = tokenizer(\n",
    "            batch[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=1024,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "        enc[\"labels\"] = enc[\"input_ids\"].copy()\n",
    "        return enc\n",
    "\n",
    "    tokenized = ds.map(tok, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=min(4, BATCH_SIZE),\n",
    "        gradient_accumulation_steps=GRAD_ACCUM,\n",
    "        learning_rate=LR,\n",
    "        warmup_ratio=WARMUP_RATIO,\n",
    "        logging_steps=LOG_STEPS,\n",
    "        save_steps=SAVE_STEPS,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=SAVE_STEPS,\n",
    "        report_to=\"none\",\n",
    "        optim=\"paged_adamw_8bit\" if USE_4BIT else \"adamw_torch\",\n",
    "        seed=SEED,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        bf16=False,\n",
    "        gradient_checkpointing=True,\n",
    "        max_steps=MAX_STEPS,   # int; 0 means use epochs\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=tokenized[\"train\"],\n",
    "        eval_dataset=tokenized[\"validation\"],\n",
    "        data_collator=default_data_collator,  # we already set labels\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    model.save_pretrained(OUTPUT_DIR)\n",
    "    print(f\"Saved LoRA adapter to {OUTPUT_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ub9LJ-67huWe",
    "outputId": "a0fb53bc-4a3e-400c-f1b1-8440c6303745"
   },
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/LLM_Assignment_2\n",
    "!python train_lora.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xMtQNWDae1Ab",
    "outputId": "171c5221-58bd-4fe7-da49-19662750277c"
   },
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/LLM_Assignment_2\n",
    "import os\n",
    "os.environ[\"BASE_MODEL\"] = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
    "os.environ[\"PEFT_ADAPTER_PATH\"] = \"adapters/task-lora\"   # your trained LoRA\n",
    "os.environ[\"EVAL_FILE\"] = \"data/eval_questions.jsonl\"\n",
    "\n",
    "!python infer.py\n",
    "!sed -n '1,5p' outputs/finetuned_responses.jsonl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IoMmusu7iH3S",
    "outputId": "6d731e71-658e-49f3-de1e-237bfc3b93ec"
   },
   "outputs": [],
   "source": [
    "!python unit_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361,
     "referenced_widgets": [
      "67babe1528b041b4a12d7c202f1718fb",
      "7c1333ddf4154162aa043e31c7ed3c79",
      "b6063a1ac994452baf4f38540833f777",
      "1091007fa07f40ae92345f290846a87f",
      "887e3a3c65ac48c9981779ba49a0a2af",
      "93b5f9a4addc4908b50cc73dae55a5b6",
      "bb97f192ebc3468ca8e02c1a108426b2",
      "c157a0566b4e4273bf7376cb2481591a",
      "eb84337147a84a86b8037e69abc5ecf3",
      "592550c9ca4340d5a18459ad5af962c1",
      "9d46c5ea0707407b8314aa89b6b8ff47",
      "d1344ea7b71142daa4aceeb0e61d7446",
      "f99a4830332746759011f2ecfc233883",
      "17b30a940a8a414f92e174646bdfb379",
      "8b6cc9d7169d48309f64c4ee24552fa2",
      "cfc4bd3dbd704e6297927051c3caf0a7",
      "c0fe14a6a28048e497d360522736bfd6",
      "e9a19ab7d23644848b2ffc2fc774daf0",
      "2ef8c6eb1f9442bdbe49574ba5f7cd40",
      "88254bb6524d40fb80dc8dd42dc78b42",
      "d5998c93c9ad42e0921e7b985fa63809",
      "85ccb1002e4741969356193390b5e39f",
      "953e47b08f0f46f5b74a08ee45067b86",
      "cddf5d1c4a614b3c8796e35ab053d934",
      "168488cacdca434988ad43773ad6f31e",
      "5c594dd8391d4ad48617b9e8b56c35dd",
      "ca03dc7a1bec437cb6cbf1641d03b5d1",
      "e5860cc26e4a4b89a4de55d17fc1f3bc",
      "e0f3b9d1452e451c866977bcbebf2528",
      "2011e06b39fd42c48542200e9a4f4a9b",
      "048c9960828a4894bb0970f1944f5ffe",
      "ce6d8f75a3f247abb10d521a921003ae",
      "c15ea975f3354860b220871d42e600dd",
      "7c2c2b01f77f481cbb06c4e4f5e53aea",
      "1620855c9e4d48058b61b07cf51541f3",
      "3407b668f6b94263bc76ea58b9bded6f",
      "457b96fdd2d34ea695f023c5bd1a0791",
      "6f7bd93e63f3488bbaf0ff15aaa2f2d8",
      "45076fdc5a7c4dd0be358731dda723ae",
      "54de16122d744561a56484b1b414d403",
      "a164e7a361c741daa1d72d7646ecfd54",
      "79da628781784bd18978b5fe90cbeea3",
      "e2acfa78d92a4f208bfafc41285636f3",
      "b39d48b82bb84582ad960116a1d9e4da",
      "8505080ea0d243b58bc6e35d91df4c01",
      "bffb2505250e401bb9f3bf2a1f8bdff6",
      "87c6efc77e3b4ebc93e061c568c59cfd",
      "635272e580be4afeacf0c1530893b640",
      "fe5ded136c3743fbb5e36b901b237878",
      "93ea9bb8003e4d0b85aa2db939774480",
      "aeb2fd7b4bb942c08b09f1537a6a0465",
      "3afd03c504ef48dc93e8bb417a3c8e30",
      "afab84a66f4d4401b9163d9fec248802",
      "ef67b0910ec3453eb4f6cc4f05b880b2",
      "d76c2348c0914f9e8117a9d7b6a2f665",
      "7b277c9c7c0f4daaaf0e13a6c2add18b",
      "5183f9cad5ac40f2abf8f69e72bc5978",
      "a15eab24c35d401abda136b16176f749",
      "28803a4107e84f799253759d3f8b967f",
      "2f98fc3887b54523aec6b7fdd757de38",
      "9cc8f774c3634a539b82ec515fd81f4b",
      "d08403b784b2486597bad108674e292b",
      "a2a91072c7ce4f1a9eb30ea4b79ab716",
      "24f8dcdc3d2a43b5bfb2b83df9fd4417",
      "19c25fa1167545a9a3631b73b16e8c5e",
      "a38a6e049d5946a18d2c8835376c1958",
      "4c69d32a1f9d45089dd8925de3e23ef9",
      "82909546f5e14743addf90076a22734b",
      "e1e4e68c4276459e8910dd02dde277c0",
      "9e04d91af295406697b39f4a086d3eac",
      "1d2e9ddf4c20469aa329acd945ef8557",
      "8b43988546c941a2ba5a1c7ccd0c9dcf",
      "dbe9948d9d3e4da98a875384aff0ff2b",
      "56a3774d42424a908b41ae4f4bbbd237",
      "b0841e40b9c643d4a65cb14cdfe0dcf3",
      "808a9972e4824f978d24165948e5ae68",
      "648406135a7f486799e22de77215abe4"
     ]
    },
    "id": "kLW-S9hXk30Y",
    "outputId": "17f287e8-adf1-47e6-9abb-35c4cae76e10"
   },
   "outputs": [],
   "source": [
    "# Build binary sentiment from TweetEval (filter out 'neutral')\n",
    "%cd /content/drive/MyDrive/LLM_Assignment_2\n",
    "!pip -q install datasets\n",
    "\n",
    "import json, random\n",
    "from datasets import load_dataset\n",
    "\n",
    "random.seed(7)\n",
    "ds = load_dataset(\"tweet_eval\", \"sentiment\")\n",
    "\n",
    "# labels: 0=negative, 1=neutral, 2=positive → keep only 0/2\n",
    "def binarize(split):\n",
    "    rows=[]\n",
    "    for x, y in zip(split[\"text\"], split[\"label\"]):\n",
    "        if y==1:\n",
    "            continue\n",
    "        rows.append({\"text\": x, \"label\": \"POS\" if y==2 else \"NEG\"})\n",
    "    return rows\n",
    "\n",
    "train_b = binarize(ds[\"train\"])\n",
    "val_b   = binarize(ds[\"validation\"])\n",
    "test_b  = binarize(ds[\"test\"])\n",
    "\n",
    "# sample sizes\n",
    "random.shuffle(train_b); random.shuffle(val_b); random.shuffle(test_b)\n",
    "N_TRAIN, N_VAL, N_EVAL = 2000, 300, 60\n",
    "train_s, val_s, eval_s = train_b[:N_TRAIN], val_b[:N_VAL], test_b[:N_EVAL]\n",
    "\n",
    "!mkdir -p data\n",
    "\n",
    "with open(\"data/train.jsonl\",\"w\") as ft:\n",
    "    for r in train_s:\n",
    "        j = {\n",
    "            \"instruction\": \"Classify sentiment as POS or NEG.\",\n",
    "            \"input\": r[\"text\"],\n",
    "            \"output\": r[\"label\"]\n",
    "        }\n",
    "        ft.write(json.dumps(j, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "with open(\"data/val.jsonl\",\"w\") as fv:\n",
    "    for r in val_s:\n",
    "        j = {\n",
    "            \"instruction\": \"Classify sentiment as POS or NEG.\",\n",
    "            \"input\": r[\"text\"],\n",
    "            \"output\": r[\"label\"]\n",
    "        }\n",
    "        fv.write(json.dumps(j, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# Eval with ground truth so we can score accuracy/F1 later\n",
    "with open(\"data/eval_questions.jsonl\",\"w\") as fe:\n",
    "    for r in eval_s:\n",
    "        j = {\n",
    "            \"instruction\": \"Classify sentiment as POS or NEG.\",\n",
    "            \"input\": r[\"text\"],\n",
    "            \"reference\": r[\"label\"]\n",
    "        }\n",
    "        fe.write(json.dumps(j, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def count_lines(p):\n",
    "    return sum(1 for _ in open(p))\n",
    "print({\n",
    "    \"train\": count_lines(\"data/train.jsonl\"),\n",
    "    \"val\":   count_lines(\"data/val.jsonl\"),\n",
    "    \"eval\":  count_lines(\"data/eval_questions.jsonl\"),\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "54biBJDDlYmb"
   },
   "outputs": [],
   "source": [
    "# Train (same env you used)\n",
    "import os\n",
    "os.environ[\"BASE_MODEL\"] = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
    "os.environ[\"TRAIN_PATH\"] = \"data/train.jsonl\"\n",
    "os.environ[\"VAL_PATH\"]   = \"data/val.jsonl\"\n",
    "os.environ[\"OUTPUT_DIR\"] = \"adapters/task-lora\"\n",
    "os.environ[\"USE_4BIT\"]   = \"1\"\n",
    "os.environ[\"LORA_R\"]     = \"16\"\n",
    "os.environ[\"LORA_ALPHA\"] = \"32\"\n",
    "os.environ[\"LORA_DROPOUT\"] = \"0.05\"\n",
    "os.environ[\"LR\"] = \"2e-4\"\n",
    "os.environ[\"EPOCHS\"] = \"1\"\n",
    "os.environ[\"BATCH_SIZE\"] = \"8\"\n",
    "os.environ[\"GRAD_ACCUM\"] = \"2\"\n",
    "os.environ[\"SAVE_STEPS\"] = \"200\"\n",
    "os.environ[\"LOG_STEPS\"]  = \"10\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YvRpBfGAlcm-",
    "outputId": "842932a0-cff2-4c22-f2ea-c51bde672707"
   },
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/LLM_Assignment_2\n",
    "!python train_lora.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0fXoC3GWlc3p",
    "outputId": "ecf67169-64c7-4f61-cbf4-e71a9847dc69"
   },
   "outputs": [],
   "source": [
    "# Inference: base vs fine-tuned\n",
    "import os\n",
    "# Base\n",
    "os.environ[\"PEFT_ADAPTER_PATH\"] = \"\"\n",
    "os.environ[\"EVAL_FILE\"] = \"data/eval_questions.jsonl\"\n",
    "!python infer.py\n",
    "\n",
    "# Finetuned\n",
    "os.environ[\"PEFT_ADAPTER_PATH\"] = \"adapters/task-lora\"\n",
    "!python infer.py\n",
    "\n",
    "!sed -n '1,3p' outputs/base_responses.jsonl\n",
    "!sed -n '1,3p' outputs/finetuned_responses.jsonl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vx2EOZN6lgLE",
    "outputId": "34049609-678f-4627-f194-a593376c067c"
   },
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/LLM_Assignment_2\n",
    "!pip -q install scikit-learn\n",
    "\n",
    "import json, re\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "POS_PAT = re.compile(r\"\\bpos(itive)?\\b\", re.I)\n",
    "NEG_PAT = re.compile(r\"\\bneg(ative)?\\b\", re.I)\n",
    "\n",
    "def normalize_to_label(text: str) -> str:\n",
    "    if not text: return \"NEG\"\n",
    "    t = str(text).strip()\n",
    "    if not t: return \"NEG\"\n",
    "    if POS_PAT.search(t): return \"POS\"\n",
    "    if NEG_PAT.search(t): return \"NEG\"\n",
    "    first = t.split()[0].strip(\" .:,-\").lower()\n",
    "    if first in {\"pos\",\"positive\",\"+\",\"1\"}: return \"POS\"\n",
    "    if first in {\"neg\",\"negative\",\"-\",\"0\"}: return \"NEG\"\n",
    "    return \"NEG\"\n",
    "\n",
    "def load_preds(path):\n",
    "    preds, empty = [], 0\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            j = json.loads(line)\n",
    "            resp = j.get(\"response\", \"\")\n",
    "            if not resp or not str(resp).strip():\n",
    "                empty += 1\n",
    "            preds.append(normalize_to_label(resp))\n",
    "    print(f\"[load_preds] {path}: {len(preds)} preds, blank = {empty}\")\n",
    "    return preds\n",
    "\n",
    "def load_refs(eval_path):\n",
    "    refs=[]\n",
    "    with open(eval_path) as f:\n",
    "        for line in f:\n",
    "            j=json.loads(line)\n",
    "            refs.append(j.get(\"reference\",\"\").strip().upper())\n",
    "    return refs\n",
    "\n",
    "# Load\n",
    "refs = load_refs(\"data/eval_questions.jsonl\")\n",
    "pred_base = load_preds(\"outputs/base_responses.jsonl\")\n",
    "pred_ft   = load_preds(\"outputs/finetuned_responses.jsonl\")\n",
    "\n",
    "# Align lengths just in case\n",
    "m = min(len(refs), len(pred_base), len(pred_ft))\n",
    "refs, pred_base, pred_ft = refs[:m], pred_base[:m], pred_ft[:m]\n",
    "\n",
    "# Map to ints\n",
    "map2i = {\"NEG\":0, \"POS\":1}\n",
    "y_true = [map2i.get(r,0) for r in refs]\n",
    "y_base = [map2i.get(p,0) for p in pred_base]\n",
    "y_ft   = [map2i.get(p,0) for p in pred_ft]\n",
    "\n",
    "def show_metrics(name, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1m = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    print(f\"\\n{name}  Accuracy: {acc:.3f}   Macro-F1: {f1m:.3f}\")\n",
    "    print(\"Confusion matrix [[TN FP],[FN TP]]:\\n\", confusion_matrix(y_true, y_pred))\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"NEG\",\"POS\"], zero_division=0))\n",
    "\n",
    "show_metrics(\"Base    \", y_base)\n",
    "show_metrics(\"Fine-tuned\", y_ft)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wXvUNx9XljSb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
