# IEMS 490-0 Assignment 2 â€” LoRA Fine-Tuning on Social Media Sentiment (TweetEval)

## Overview
This assignment implements **parameter-efficient fine-tuning (LoRA)** on a small language model to perform **sentiment classification** on the **TweetEval** dataset (Positive / Negative).  
The goal is to compare the base modelâ€™s performance with the fine-tuned LoRA adapter and analyze the results.

---

## Repository Structure
```
IEMS_490-0_Assignment2/
â”‚
â”œâ”€â”€ adapters/                       # Trained LoRA adapters & checkpoints
â”‚   â”œâ”€â”€ task-lora/checkpoint-1/     # First training checkpoint (LoRA)
â”‚   â””â”€â”€ _unit_lora/checkpoint-20/   # Final training checkpoint (20 epochs)
â”‚
â”œâ”€â”€ data/                           # TweetEval data splits
â”‚   â”œâ”€â”€ train.jsonl
â”‚   â”œâ”€â”€ val.jsonl
â”‚   â””â”€â”€ eval_questions.jsonl
â”‚
â”œâ”€â”€ outputs/                        # Model inference results
â”‚   â”œâ”€â”€ base_responses.jsonl
â”‚   â””â”€â”€ finetuned_responses.jsonl
â”‚
â”œâ”€â”€ tmp_unit/                       # Small-scale LoRA test runs
â”‚
â”œâ”€â”€ train_lora.py                   # LoRA fine-tuning script
â”œâ”€â”€ infer.py                        # Inference & generation for evaluation
â”œâ”€â”€ unit_test.py                    # Unit-level dataset / eval sanity checks
â”œâ”€â”€ LLM_Assignment2.ipynb # End-to-end notebook (Colab/DeepDish)
â””â”€â”€ README.md                       # You are here
```
---

## Environment Setup

```bash
pip install -r requirements.txt
pip install torch transformers peft datasets evaluate scikit-learn accelerate
```

## How to Run
1. Fine-Tuning with LoRA
```bash
python train_lora.py \
--model HuggingFaceTB/SmolLM2-360M-Instruct \
--train_file data/train.jsonl \
--validation_file data/val.jsonl \
--output_dir adapters/task-lora \
--num_train_epochs 1 \
--per_device_train_batch_size 4 \
--learning_rate 1e-4
```
2. Generating Responses (Base + Fine-Tuned)
```bash
python infer.py \
  --model HuggingFaceTB/SmolLM2-360M-Instruct \
  --adapter adapters/task-lora/checkpoint-1 \
  --eval_file data/eval_questions.jsonl \
  --output_dir outputs/
```
3. Evaluating Results
```bash
python unit_test.py
```
This computes accuracy, macro-F1, and prints the confusion matrix for both models.

---
## Notebook: LLM_Assignment2.ipynb

Option A â€” Run in Google Colab (T4 GPU)

1. Open the notebook in Colab (Runtime â†’ Change runtime type â†’ GPU (T4)).
2. Run the Setup cell; it will:
   - create ~/drive/MyDrive/LLM_Assignment_2 if needed,
   - install dependencies,
   - place outputs in outputs/ and adapters in adapters/.
3. Run all cells (Runtime â†’ Run all).
   - The notebook executes: data prep â†’ training â†’ inference â†’ metrics.

Handy Colab shell cells you can use if needed:
```bash
# Create the working folder in Drive (if you want to mirror CLI layout)
mkdir -p /content/drive/MyDrive/LLM_Assignment_2
```
Option B â€” Run on DeepDish via Notebook

If you prefer a notebook on DeepDish:
```bash
module load python/3.10
python -m venv ~/venvs/llm && source ~/venvs/llm/bin/activate
pip install jupyterlab torch transformers peft datasets evaluate scikit-learn accelerate

# (Option 1) Start JupyterLab and port-forward (recommended in NU docs)
jupyter lab --no-browser --port 8888

# (Option 2) Execute the notebook headlessly
pip install papermill
papermill LLM_Assignment2.ipynb LLM_Assignment2.out.ipynb
```


## Results

| Model | Accuracy | Macro F1 |
|:------|:---------:|:---------:|
| **Base (SmolLM2-360M-Instruct)** | **0.617** | **0.611** |
| **Fine-Tuned (LoRA)** | **0.583** | **0.569** |

---

### Confusion Matrices

**Base Model**
```
[[22 16]
 [ 7 15]]
``` 
**Fine-Tuned Model**
```
[[23 15]
 [10 12]]
```
---

## Sample Outputs

| Input Tweet | Base Prediction | Fine-Tuned Prediction | Gold |
|--------------|----------------|----------------------|------|
| â€œHad a great time at the concert tonight!â€ | POS | POS | POS |
| â€œTraffic jam againâ€¦ this city sucks.â€ | NEG | NEG | NEG |
| â€œI canâ€™t believe how amazing this food is!â€ | POS | POS | POS |
| â€œMy flight got delayed for 4 hours, wonderful ğŸ™„â€ | NEG | POS | NEG |
| â€œTotally worth staying up all night for this project.â€ | POS | NEG | POS |

---

## Discussion â€” Base vs Fine-Tuned Models

- **Base Model** (`SmolLM2-360M-Instruct`) already exhibits strong **sentiment understanding** due to large-scale pretraining.  
- **Fine-Tuned Model** learns domain-specific tweet nuances but shows mild **overfitting**, leading to a slight accuracy drop (âˆ’3 pts).  
- The fine-tuned model performs better on **sarcastic or informal** tweets but is **weaker on balanced or ambiguous** ones.  
- With additional training (2â€“3 epochs) or a larger dataset (â‰¥ 2,000 samples), the LoRA fine-tuned model would likely surpass the base model.

---

## Running on DeepDish GPU (Northwestern HPC)

You can run the same training workflow on **DeepDish GPU nodes**:

```bash
# 1. Load environment
module load python/3.10
source ~/venvs/llm/bin/activate

# 2. Clone your repo and move to directory
git clone https://github.com/Shruti722/IEMS_490-0_Assignment2.git
cd IEMS_490-0_Assignment2

# 3. Request GPU node
srun --gpus=1 --mem=32G --time=2:00:00 --pty bash

# 4. Run LoRA training
python train_lora.py --train_file data/train.jsonl --validation_file data/val.jsonl --output_dir adapters/task-lora

# 5. Run evaluation
python unit_test.py
```
---

## Key Takeaways

- Implemented a complete LoRA fine-tuning pipeline on TweetEval.
- Compared base vs fine-tuned model performance using accuracy, macro-F1, and confusion matrices.
- Observed consistent classification trends between positive and negative sentiments.
- Demonstrated reproducibility across Colab and DeepDish GPU environments.
